hyperparameters:
  learning_rate : 1e-3
  dropout : 0.1
  size_of_attention_head : 4
  num_of_attention_head : 4
  n_embd : 64
  batch_size : 1024
  block_size : 256

training:
  epochs : 600
  patience : 10
  model_checkpoint_path : "language_model.pt"

dataset:
  dataset_file_name : "dataset.txt"

max_new_tokens : 100
