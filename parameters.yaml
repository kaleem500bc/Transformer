hyperparameters:
  learning_rate : 1e-3
  dropout : 0.1
  size_of_attention_head : 4
  num_of_attention_head : 4
  n_embd : 64

training:
  training_epochs : 10
  eval_patience : 2
  batch_size : 256
  block_size : 64
  model_checkpoint_path : "language_model.pt"

dataset:
  output_file_name : "dataset.txt"

#maximum token the model is allowed to generate
max_new_tokens : 100
